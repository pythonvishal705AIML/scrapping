# -*- coding: utf-8 -*-
"""Scraping_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0ads4sQjGhjUUEl4kR4a86cNhj088ww
"""

import requests
from bs4 import BeautifulSoup
import csv
from datetime import date

url = "https://www.moneycontrol.com/india/stockpricequote/auto-lcvshcvs/tatamotors/TM03"
response = requests.get(url)

#Fetch the HTML content of the specified webpage

soup.find('table')

#Locate and return the first <table> element

soup.find_all('table')[1]
# Retrieve the second <table> element

soup = BeautifulSoup(response.content, 'html.parser')
headings = []
for i in range(1, 7):
    for heading in soup.find_all(f'h{i}'):
        headings.append((heading.name, heading.text.strip()))
print(headings)

#extract all headings (h1 to h6) with their text, and store them in a list

table = soup.find('div', class_='oview_table')

rows = table.find_all('tr')

for row in rows:
    cells = row.find_all(['td', 'th'])  # Extract both table data and header cells
    cell_data = [cell.get_text(strip=True) for cell in cells]
    print(cell_data)

#Finding table within specified div, extract all rows, and print the text content

if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    def get_text_from_label(label):
        cell = soup.find("td", string=label)
        return cell.find_next_sibling("td").text.strip() if cell else "N/A"

    open_price = get_text_from_label("Open")
    high_price = get_text_from_label("High")
    low_price = get_text_from_label("Low")
    close_price = get_text_from_label("Previous Close")
    volume = get_text_from_label("Volume")

    share_name_full = url.split("/")[-2]
    share_name = " ".join(share_name_full.split(" ")[:2])

    today = date.today().strftime("%Y-%m-%d")

    data = [
        [share_name, today, open_price, high_price, low_price, close_price, volume]
    ]

    with open("stock_data.csv", "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Share Name", "Date", "Open", "High", "Low", "Close", "Volume"])
        writer.writerows(data)

# parse the HTML, extract stock data (Open, High, Low, Previous Close, Volume) and save i think this is most imp info from thsi page

overview_section = soup.find('div', class_='overview_section')
data = []
if overview_section:
    rows = overview_section.find_all('tr')
    for row in rows:
        cells = row.find_all(['td', 'th'])
        cell_data = [cell.get_text(strip=True) for cell in cells]
        data.append(cell_data)
data

for i, row in enumerate(data):
    print(f"Row {i}: {row}")


    df = pd.DataFrame(consistent_data, columns=headers)
    print(df)

    df.to_csv('overview_section_data.csv', index=False)
# Extract data from the 'overview_section', store it in a list

if data:
    headers = data[0]
    print(f"Number of headers: {len(headers)}")

    consistent_data = [row for row in data[1:] if len(row) == len(headers)]

    for i, row in enumerate(data[1:]):
        if len(row) != len(headers):
            print(f"Skipping Row {i + 1}: {row}")
            continue
        print(f"Row {i + 1}: {row}")

    df = pd.DataFrame(consistent_data, columns=headers)
    print(df)

    df.to_csv('overview_section_data.csv', index=False)

#filter consistent rows (matching header length) and print any rows that are skipped due to length mismatch.

df



